{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import json\n",
    "import scipy.sparse as ss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC \n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998645 1998645\n"
     ]
    }
   ],
   "source": [
    "# loading the newdata file \n",
    "newdata = [w.strip().split('\\t') for w in open('newdata/mcic-coco/data/mc_mscoco_train_foilformat.txt')]\n",
    "\n",
    "# loading gold image features \n",
    "ourimagetrainfeats = [((w.strip().split()[0]), np.array(map(float, w.strip().split()[1:]))) \n",
    "                      for w in open('data/mscoco_boc_gt_train2014.txt')]\n",
    "\n",
    "# converting into a dictionary as image feats are constant across 5 different captions / image\n",
    "ourimagetrainfeats = dict(ourimagetrainfeats)\n",
    "\n",
    "# getting images\n",
    "newdataimages = [ourimagetrainfeats[l[0]] for l in newdata]\n",
    "\n",
    "# Retaining Captions\n",
    "training_annotations = [l[1] for l in newdata]\n",
    "\n",
    "training_labels = [l[2] for l in newdata]\n",
    "\n",
    "# Loading MSCOCO object category names\n",
    "categories = [w.strip().split('\\t')[1] for w in open('data/categories.txt')]\n",
    "\n",
    "# Printing dataset samples \n",
    "print len(training_annotations), len(newdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two girls and one boy playing video games .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using tf-idf based feature extractor\n",
    "tf_vectorizer = CountVectorizer(max_features=None, lowercase=True) # Use the full training vocab and lowercase\n",
    "\n",
    "# Fit the training dataset and save the model \n",
    "tf_model = tf_vectorizer.fit(training_annotations) \n",
    "\n",
    "# Obtain training features - this is a sparse matrix\n",
    "training_feats = tf_model.transform(training_annotations)\n",
    "\n",
    "# Obtain the outputs (0/1)\n",
    "training_y = [0 if w == 'REAL' else 1 for w in training_labels]\n",
    "\n",
    "# just obtaining the image features - iterating over image id and just saving in the list\n",
    "#training_image_feats = [ourimagetrainfeats[i['image_id']] for i in foil_train['annotations']]\n",
    "training_image_feats = newdataimages\n",
    "\n",
    "# converting into a sparse matrix\n",
    "training_image_feats_sparse = ss.csr_matrix(np.array(training_image_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'and', u'boy', u'games', u'girls', u'one', u'playing', u'two', u'video']\n",
      "Original Sentence:  two girls and one boy playing video games .\n",
      "VALUE:  1 FAKE\n"
     ]
    }
   ],
   "source": [
    "# Printing the exact bag of words active for the first sample\n",
    "print np.array(tf_model.get_feature_names())[([np.array(training_feats[0].todense()) > 0][0][0]).tolist()].tolist()\n",
    "\n",
    "# printing the original sentence\n",
    "print 'Original Sentence: ',   training_annotations[0]\n",
    "\n",
    "# checking if Y is correct -> Original = 0 and Fake = 1 \n",
    "print 'VALUE: ', training_y[0], training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the file \n",
    "newtestdata = [w.strip().split('\\t') for w in open('newdata/mcic-coco/data/mc_mscoco_test_foilformat.txt')]\n",
    "\n",
    "\n",
    "# loading gold image features \n",
    "ourimagetestfeats = [((w.strip().split()[0]), np.array(map(float, w.strip().split()[1:]))) \n",
    "                      for w in open('data/mscoco_boc_gt_train2014.txt')]\n",
    "\n",
    "# retaining captions \n",
    "testing_annotations = [l[1] for l in newtestdata]\n",
    "\n",
    "\n",
    "\n",
    "# converting into dict\n",
    "ourimagetestfeats = dict(ourimagetestfeats)\n",
    "\n",
    "# testing labels\n",
    "testing_labels = [l[2] for l in newtestdata]\n",
    "\n",
    "\n",
    "\n",
    "# using the model that was fit on the training data, extracting the test features on the test data\n",
    "testing_feats = tf_model.transform(testing_annotations) \n",
    "\n",
    "# test outputs \n",
    "testing_y = [0 if w == 'REAL' else 1 for w in testing_labels]                    \n",
    "\n",
    "# ust obtaining the image features - iterating over image id and just saving in the list\n",
    "testing_image_feats = [ourimagetrainfeats[l[0]] for l in newtestdata]\n",
    "\n",
    "#converting into a sparse array\n",
    "testing_image_feats_sparse = ss.csr_matrix(np.array(testing_image_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only Language Features: \n",
    "\n",
    "# just putting into a standard X_train, X_test, Y_train, Y_test stuff\n",
    "X_train = training_feats\n",
    "X_test = testing_feats\n",
    "\n",
    "Y_train = np.array(training_y)\n",
    "Y_test = np.array(testing_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['FAKE', 'FAKE', 'FAKE', 'FAKE', 'REAL'], 1598916, 1998645, 29328, 36660)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels[:5], sum(Y_train), len(Y_train), sum(Y_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8058101473\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       REAL     0.5516    0.1552    0.2423      7332\n",
      "       FAKE     0.8210    0.9685    0.8886     29328\n",
      "\n",
      "avg / total     0.7671    0.8058    0.7594     36660\n",
      "\n",
      "[('REAL', 0.15521003818876158), ('FAKE', 0.96846017457719591)]\n"
     ]
    }
   ],
   "source": [
    "# using a MultiLayerPerceptron model - default settings\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "\n",
    "# fitting over training data\n",
    "mlp.fit(X_train, Y_train)\n",
    "\n",
    "# printing overall accuracy \n",
    "print 'Accuracy = ', metrics.accuracy_score(Y_test, mlp.predict(X_test.toarray()))\n",
    "\n",
    "# printing the precision and recall over each class \n",
    "target_names = ['REAL', 'FAKE']\n",
    "print metrics.classification_report(Y_test, mlp.predict(X_test.toarray()), \n",
    "                                    target_names=target_names, digits=4)\n",
    "\n",
    "# printing accuracy over each class\n",
    "cmat = metrics.confusion_matrix(Y_test, mlp.predict(X_test.toarray()))\n",
    "print zip(target_names, cmat.diagonal()/cmat.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.685"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.82 + 0.55) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.589934533552\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       REAL     0.1523    0.2300    0.1832      7332\n",
      "       FAKE     0.7793    0.6799    0.7262     29328\n",
      "\n",
      "avg / total     0.6539    0.5899    0.6176     36660\n",
      "\n",
      "[('REAL', 0.22995090016366612), ('FAKE', 0.67993044189852703)]\n"
     ]
    }
   ],
   "source": [
    "# using a DecisionTreeClassifier \n",
    "dtc = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "# fitting over training data\n",
    "dtc.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# printing overall accuracy \n",
    "print 'Accuracy = ', metrics.accuracy_score(Y_test, dtc.predict(X_test.toarray()))\n",
    "\n",
    "# printing the precision and recall over each class \n",
    "target_names = ['REAL', 'FAKE']\n",
    "print metrics.classification_report(Y_test, dtc.predict(X_test.toarray()), \n",
    "                                    target_names=target_names, digits=4)\n",
    "\n",
    "# printing accuracy over each class\n",
    "cmat = metrics.confusion_matrix(Y_test, dtc.predict(X_test.toarray()))\n",
    "print zip(target_names, cmat.diagonal()/cmat.sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only Image Features\n",
    "\n",
    "X_train = training_image_feats\n",
    "X_test = testing_image_feats\n",
    "\n",
    "Y_train = np.array(training_y)\n",
    "Y_test = np.array(testing_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       REAL     0.0000    0.0000    0.0000      7332\n",
      "       FAKE     0.8000    1.0000    0.8889     29328\n",
      "\n",
      "avg / total     0.6400    0.8000    0.7111     36660\n",
      "\n",
      "[('REAL', 0.0), ('FAKE', 1.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranava/miniconda3/envs/pytorch2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# using a MultiLayerPerceptron model - default settings\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "mlp.fit(X_train, Y_train)\n",
    "print 'Accuracy = ', metrics.accuracy_score(Y_test, mlp.predict(X_test))\n",
    "target_names = ['REAL', 'FAKE']\n",
    "print metrics.classification_report(Y_test, mlp.predict(X_test), \n",
    "                                    target_names=target_names, digits=4)\n",
    "cmat = metrics.confusion_matrix(Y_test, mlp.predict(X_test))\n",
    "print zip(target_names, cmat.diagonal()/cmat.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text and image features\n",
    " \n",
    "X_train = ss.hstack([(training_feats), training_image_feats_sparse])\n",
    "X_test = ss.hstack([(testing_feats), testing_image_feats_sparse])\n",
    "\n",
    "Y_train = np.array(training_y)\n",
    "Y_test = np.array(testing_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.813502454992\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       REAL     0.6118    0.1847    0.2837      7332\n",
      "       FAKE     0.8265    0.9707    0.8928     29328\n",
      "\n",
      "avg / total     0.7835    0.8135    0.7710     36660\n",
      "\n",
      "[('REAL', 0.18466993998908893), ('FAKE', 0.97071058374249863)]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "mlp.fit(X_train, Y_train)\n",
    "print 'Accuracy = ', metrics.accuracy_score(Y_test, mlp.predict(X_test.toarray()))\n",
    "target_names = ['REAL', 'FAKE']\n",
    "print metrics.classification_report(Y_test, mlp.predict(X_test.toarray()), \n",
    "                                    target_names=target_names, digits=4)\n",
    "cmat = metrics.confusion_matrix(Y_test, mlp.predict(X_test.toarray()))\n",
    "print zip(target_names, cmat.diagonal()/cmat.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     2,      3,      7, ..., 150553, 150554, 150555])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testmat = X_test.toarray()\n",
    "carr = np.array(range(len(Y_test)))\n",
    "carr[Y_test > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1196, 10109, 11385, 11455, 13985, 17267, 17917])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rarr = np.array(range(testmat.shape[1]))\n",
    "rarr[(testmat[2] > 0).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict_proba(testmat[2].reshape(-1,1).T).argmax() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmat = testmat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmat[2, 17267] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmat[2, 1196] = 1\n",
    "tmat[2, 10109] = 1\n",
    "tmat[2, 11385] = 1\n",
    "tmat[2, 11455] = 1\n",
    "tmat[2, 17917] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85962384428013072"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict_proba(tmat[2].reshape(-1,1).T)[0][1] - mlp.predict_proba(tmat[2].reshape(-1,1).T)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'back', u'man', u'of', u'on', u'riding', u'the', u'truck']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf_model.get_feature_names())[([np.array(testing_feats[2].todense()) > 0][0][0]).tolist()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'truck'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.get_feature_names()[17917]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_foil(X, Y, model):\n",
    "    \n",
    "    tmat = X.toarray()\n",
    "    \n",
    "    carr = np.array(range(len(Y)))\n",
    "    yvals = carr[Y_test > 0]\n",
    "    \n",
    "    \n",
    "    rarr = np.array(range(tmat.shape[1]))\n",
    "    retvals = [] \n",
    "    for s in tqdm(yvals):\n",
    "        tmpdict = {} \n",
    "        marr = tmat[s]\n",
    "        mvals = rarr[(marr > 0).tolist()]\n",
    "        for w in mvals:\n",
    "            tmp = marr[w]\n",
    "            marr[w] = 0 \n",
    "            tmpdict[w] = model.predict_proba(marr.reshape(-1, 1).T)[0][1] - model.predict_proba(marr.reshape(-1, 1).T)[0][0]\n",
    "            marr[w] = tmp\n",
    "            \n",
    "        sorted_tmpdict = sorted(tmpdict.items(), key=itemgetter(1))\n",
    "#        from IPython.core.debugger import Tracer; Tracer()() \n",
    "        retvals.append(sorted_tmpdict[0][0])\n",
    "        \n",
    "    return retvals\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75278/75278 [11:36<00:00, 108.07it/s]\n"
     ]
    }
   ],
   "source": [
    "retvals = detect_foil(X_test, Y_test, mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75278, 75278)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(carr[Y_test > 0]), len(retvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17917, 17729, 800, 2857, 7806, 18122, 1225, 18036, 17522, 15257]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featnames = tf_model.get_feature_names() \n",
    "farr = []\n",
    "for i in carr[Y_test > 0]:\n",
    "    farr.append(featnames.index(foil_test['annotations'][i]['foil_word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75278, 75278)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retvals), len(farr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953532240495231"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(retvals, farr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
